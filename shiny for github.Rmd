---
title: "shiny-final"
author: "Yuzhou Tu"
date: "02/28/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,install packages}
##must install the following packages
install.packages('devtools')
library(devtools)
install_github("nik01010/dashboardthemes")
devtools::install_github("rstudio/pool") #it may require you upgrade some packages
##Please install those packages you don't have
#install.packages("topicmodels")
#install.packages("RSQLite")
#install.packages("shiny")
#install.packages("DBI")
#install.packages("grid")
#install.packages('rsconnect')
#install.packages("DT")
#install.packages("tokenizers")
#install.packages("reshape2")
#install.packages("tidyr")
#install.packages("magrittr")
#install.packages("tm")
#install.packages("wordcloud")
#install.packages("dplyr")
#install.packages("stringr")
#install.packages("igraph")
#install.packages("ggraph")
#install.packages("shinydashboard")
#install.packages("tidytext")
#install.packages("ggplot")
```

```{r,global}
library(RSQLite)#Create a connection to a SQLite Database
library(DT)#Create table in Shiny
library(shinydashboard)#Shiny App
library(rsconnect)#Shiny App
library(shiny)#Shiny App
library(dashboardthemes)
library("RSQLite")#Create a connection to a SQLite Database
library(pool)#Enables the creation of object pools, which make it less computationally expensive to fetch a new object. Currently the only supported pooled objects are 'DBI' connections.
library(tm)#Text mining
library(topicmodels)#Topic modeling
library(wordcloud)#Word cloud
library(dplyr)#Data wrangling
library(stringr)#Data wrangling
library(tokenizers)#Convert natural language text into tokens.
library(tidytext)#Convert text to tidy formats
library(magrittr)#To use pipe operators like %>%
library(tidyr)#Seperate bigrams to clean stopwords
library(reshape2)#Use acast
library(grid)#Draw graph
library(ggplot)#Draw graph
library(igraph)#Draw graph
library(ggraph)#Draw graph
get_sentiments("bing")
pool <- dbPool(
  drv = RSQLite :: SQLite(),
  dbname = "fake.db" #please pay attention to this directory if error occured
)
#Define 6 groups
group <<- list("Trump Support" = "Trump Support",
               "Conservative" = "Conservative",
               "Hard Conservative"="Hard Conservative",
               "Intl Right|Anti-Islam"="Intl Right|Anti-Islam",
               "Other"="Other",
               "Russia" = "Russia",
               "All" = "All")
data(stop_words)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
SEED = 2018;
#create function to clean tweet corpus 
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(tolower))#transfer to lower case
  corpus <- tm_map(corpus, removeWords, stopwords("en"))#remove stopwords
  corpus <- tm_map(corpus, stripWhitespace)#remove extra whitespace 
  corpus <- tm_map(corpus, stemDocument)#stem words
  return(corpus)
}
```


```{r,ui}
header <- dashboardHeader(title = "Propaganda Twitter Accounts Spectator",titleWidth = 380)
sidebar <- dashboardSidebar(
  width = 380,
    sidebarMenu(id="menu1",
      menuItem("Text Mining", tabName = "tm", icon = icon("dashboard"),menuItem("Summary Statistics", tabName = "ss"),menuItem("Word Clouds", tabName = "wc"),menuItem("Sentiment Analysis", tabName = "sa"),menuItem("Bigram Analysis", tabName = "ba")),
      menuItem("Topic Modeling", icon = icon("th"), tabName = "topic",
               badgeLabel = "new", badgeColor = "green")),
    conditionalPanel(condition = "input.menu1=='ss'",
      dateInput("date1", "From Date:",value='2016-10-01'),
      dateInput("date2", "To Date:",value='2016-12-01')),
    conditionalPanel(condition = "input.menu1=='wc'",
      selectInput("group", "Choose a group:",choices = group),
      dateInput("date1", "From Date:",value='2016-10-01'),
      dateInput("date2", "To Date:",value='2016-12-01'),
      hr(),
      sliderInput("freq",
                  "Minimum Frequency:",
                  min = 1,  max = 2000, value = 8),
      sliderInput("max",
                  "Maximum Number of Words:",
                  min = 1,  max = 500,  value = 100)),
    conditionalPanel(condition = "input.menu1=='sa'",
      selectInput("group_sa", "Choose a group:",choices = group),
      dateInput("date1_sa", "From Date:",value='2016-10-01'),
      dateInput("date2_sa", "To Date:",value='2016-12-01'),
      hr(),
      sliderInput("max_sa",
                  "Maximum Number of Words:",
                  min = 1,  max = 500,  value = 100)),
    conditionalPanel(condition = "input.menu1=='ba'",
      selectInput("group_ba", "Choose a group:",choices = group),
      dateInput("date1_ba", "From Date:",value='2016-10-01'),
      dateInput("date2_ba", "To Date:",value='2016-12-01'),
      hr(),
      sliderInput("freq_ba",
                  "Minimum Frequency:",
                  min = 1,  max = 100, value = 8)),
    conditionalPanel(condition = "input.menu1 == 'topic'",
      selectInput("group_topic", "Choose a group:",choices = group),
      dateInput("date3", "From Date:",value='2016-10-01'),
      dateInput("date4", "To Date:",value='2016-11-01'),
      hr(),
      numericInput("topic", label = h3("Number of topics"), value =2,max=10),
      numericInput("words", label = h3("Top n words in each topic"), value =10,max = 30),
      downloadButton('downloadData', 'Download')
    )
)
body <- dashboardBody(
  shinyDashboardThemes(
      theme = "blue_gradient"
    ),
  tags$head(tags$style(HTML('
      .main-header .logo {
        font-family: "Georgia", Times, "Times New Roman", serif;
        font-style: italic; 
        font-weight: bold;
        font-size: 16px;
      }
    '))),
  tabItems(
  tabItem(tabName='ss',
          titlePanel("Summary Statistics"),
          fluidRow(dataTableOutput("Summary"))),
  tabItem(tabName='wc',
          titlePanel("Word Clouds"),
          fluidRow(plotOutput("plot"))),
  tabItem(tabName='sa',
          titlePanel("Sentiment Analysis"),
          fluidRow(box(plotOutput("plot3",height=500),status = 'primary',solidHeader = T),box(plotOutput("plot2",height=500),status='warning',solidHeader = T))),
  tabItem(tabName='ba',
          titlePanel("Bigram Analysis"),
          fluidRow(plotOutput("plot4"))),
  tabItem(tabName ='topic',
          titlePanel("Top Words in Each Topic"),
          fluidRow(dataTableOutput('table')))
          ))
ui <- dashboardPage(header,sidebar,body)
```

```{r,server}
server <- function(input, output, session) {
  ##Part 1:summary table to summarize number of tweets sent by users from each group
  output$Summary = renderDataTable({
    sql <- "SELECT text,user_id,group_name FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2;"
    query <- sqlInterpolate(pool,sql,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
    df <- dbGetQuery(pool, query)
    #produce table
    datatable(as.data.frame(table(df$group_name)),colnames = c("Group", "Number of Tweets"),options=list(lengthMenu = c(3, 6, 9),pageLength=6))
})
  ##Part 2:word cloud
  output$plot <- renderPlot({
    if (input$group != "All"){
      sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
      query <- sqlInterpolate(pool, sql, group = input$group,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
      df <- dbGetQuery(pool, query)
      text <- df$text
      text <- as.character(text)
      text <- iconv(text, 'utf-8', 'ascii', sub='') #convert character vector between encodings
      #remove all the retweet username and urls contained in tweets
      clean_tweet = gsub("&amp", "", text)
      clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
      clean_tweet = gsub("@\\w+", "", clean_tweet)
      clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
      clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
      clean_tweet = gsub("http\\w+", "", clean_tweet)
      clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
      clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
      #create corpus using cleaned tweets
      tweet_source <- VectorSource(clean_tweet)
      tweet_corpus <- VCorpus(tweet_source)
      tweet_corpus <- clean_corpus(tweet_corpus)
      #create term-document matrix
      myTDM = TermDocumentMatrix(tweet_corpus,control = list(minWordLength = 1))
      m = as.matrix(myTDM)
      #draw word cloud
      term_frequency<-rowSums(m)
      term_frequency<-sort(term_frequency,decreasing=TRUE)
      word_freqs<-data.frame(term=names(term_frequency),num=term_frequency)
      wordcloud(word_freqs$term, 
                word_freqs$num, 
                min.freq = input$freq,#input the setted min freq
                max.words = input$max,#input the setted max number of words
                colors = brewer.pal(8, "Dark2"))
    }    
    else{
      sql <- "SELECT text,group_name FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2;"
      query <- sqlInterpolate(pool, sql,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
      df <- dbGetQuery(pool, query)
      unique_labels <- unique(df$group_name)
      text <- df$text
      text <- as.character(text)
      text <- iconv(text, 'utf-8', 'ascii', sub='') #convert character vector between encodings
      #remove all the retweet username and urls contained in tweets
      clean_tweet = gsub("&amp", "", text)
      clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
      clean_tweet = gsub("@\\w+", "", clean_tweet)
      clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
      clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
      clean_tweet = gsub("http\\w+", "", clean_tweet)
      clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
      clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
      df['text']=clean_tweet
      dataset_s <- sapply(unique_labels,function(label) paste(df$text[df$group_name==label],
                                                               collapse =" "))
      data_source <- VectorSource(dataset_s)
      data_corpus <- VCorpus(data_source)
      data_corpus <- clean_corpus(data_corpus)
      myTDM = TermDocumentMatrix(data_corpus,control = list(minWordLength = 1))
      m = as.matrix(myTDM)
      colnames(m) = unique_labels
      comparison.cloud(m,max.words=input$max, random.order=FALSE,c(4,0.4), title.size=1.4)
    }
  })
  ##Part 3:Sentiment Analysis-word cloud
    output$plot2 <- renderPlot({
      if (input$group_sa != "All"){
      sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
      query <- sqlInterpolate(pool, sql, group = input$group_sa,date1 = format(input$date1_sa,format = "%Y-%m-%d"),date2=format(input$date2_sa,format="%Y-%m-%d"))
      }
      else {
      sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2;"
      query <- sqlInterpolate(pool, sql,date1 = format(input$date1_sa,format = "%Y-%m-%d"),date2=format(input$date2_sa,format="%Y-%m-%d"))
      }
      df <- dbGetQuery(pool, query)
      tweet <- tbl_df(df)#create a dataframe table
      text2 <- as.character(tweet$text)
      text2 <- iconv(text2, 'utf-8', 'ascii', sub='')
      clean_tweet2 = gsub("&amp", "", text2)
      clean_tweet2 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet2)
      clean_tweet2 = gsub("@\\w+", "", clean_tweet2)
      clean_tweet2 = gsub("[[:punct:]]", "", clean_tweet2)
      clean_tweet2 = gsub("[[:digit:]]", "", clean_tweet2)
      clean_tweet2 = gsub("http\\w+", "", clean_tweet2)
      clean_tweet2 = gsub("[ \t]{2,}", "", clean_tweet2)
      clean_tweet2 = gsub("^\\s+|\\s+$", "", clean_tweet2)
      tweet$text <- clean_tweet2
      #convert dataframe tweet into tidytext
      tidy_tweet <- tweet %>%
        unnest_tokens(word, text)
      #remove word 'trump' from tidytext
      tidy_tweet <- tidy_tweet %>%
        anti_join(stop_words)%>%
        filter(word != "trump")
      #create sentiment word cloud
      tidy_tweet %>%
        inner_join(get_sentiments("bing")) %>%
        count(word, sentiment, sort = TRUE) %>%
        acast(word ~ sentiment, value.var = "n", fill = 0) %>%
        comparison.cloud(colors = c("gray20", "tomato"),
                         max.words = input$max_sa)#set max number of words to be included
    })
    ##Part 4: Sentiment Analysis-histogram
    output$plot3 <- renderPlot({
      if (input$group_sa != "All"){
        sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
        query  <- sqlInterpolate(pool, sql, group = input$group_sa,date1 = format(input$date1_sa,format = "%Y-%m-%d"),date2=format(input$date2_sa,format="%Y-%m-%d"))
      }
      else {
        sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2;"
        query  <- sqlInterpolate(pool, sql,date1 = format(input$date1_sa,format = "%Y-%m-%d"),date2=format(input$date2_sa,format="%Y-%m-%d"))
      }
      df <- dbGetQuery(pool, query)
      tweet3 <- tbl_df(df)
      text3 <- as.character(tweet3$text)
      text3 <- iconv(text3, 'utf-8', 'ascii', sub='')
      clean_tweet3 = gsub("&amp", "", text3)
      clean_tweet3 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet3)
      clean_tweet3 = gsub("@\\w+", "", clean_tweet3)
      clean_tweet3 = gsub("[[:punct:]]", "", clean_tweet3)
      clean_tweet3 = gsub("[[:digit:]]", "", clean_tweet3)
      clean_tweet3 = gsub("http\\w+", "", clean_tweet3)
      clean_tweet3 = gsub("[ \t]{2,}", "", clean_tweet3)
      clean_tweet3 = gsub("^\\s+|\\s+$", "", clean_tweet3)
      tweet3$text <- clean_tweet3
      tidy_tweet <- tweet3 %>%
        unnest_tokens(word, text)
      tidy_tweet <- tidy_tweet %>%
        anti_join(stop_words)%>%
        filter(word != "trump")
      bing_word_counts <- tidy_tweet %>%
        inner_join(get_sentiments("bing")) %>%
        count(word, sentiment, sort = TRUE) %>%
        ungroup()
      #plot histogram
      bing_word_counts %>%
        group_by(sentiment) %>% 
        top_n(input$max_sa/2) %>%
        ungroup() %>%
        mutate(word = reorder(word, n)) %>%
        ggplot(aes(word, n, fill = sentiment)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(~sentiment, scales = "free_y") +
        labs(y = "Contribution to sentiment",
             x = NULL) +
        coord_flip()
    })
    #Part 5:Bigram Analysis
    output$plot4 <- renderPlot({
      if (input$group_ba != "All"){
        sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
        query  <- sqlInterpolate(pool, sql, group = input$group_ba,date1 = format(input$date1_ba,format = "%Y-%m-%d"),date2=format(input$date2_ba,format="%Y-%m-%d"))
      }
      else {
        sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2;"
        query  <- sqlInterpolate(pool, sql,date1 = format(input$date1_ba,format = "%Y-%m-%d"),date2=format(input$date2_ba,format="%Y-%m-%d"))
      }
      df <- dbGetQuery(pool, query)
      tweet4 <- tbl_df(df)
      text4 <- as.character(tweet4$text)
      text4 <- iconv(text4, 'utf-8', 'ascii', sub='')
      clean_tweet4 = gsub("&amp", "", text4)
      clean_tweet4 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet4)
      clean_tweet4 = gsub("@\\w+", "", clean_tweet4)
      clean_tweet4 = gsub("[[:punct:]]", "", clean_tweet4)
      clean_tweet4 = gsub("[[:digit:]]", "", clean_tweet4)
      clean_tweet4 = gsub("http\\w+", "", clean_tweet4)
      clean_tweet4= gsub("[ \t]{2,}", "", clean_tweet4)
      clean_tweet4 = gsub("^\\s+|\\s+$", "", clean_tweet4)
      tweet4$text <- clean_tweet4
      #seperate tweets into bigrams
      tweet_bigrams <- tweet4 %>%
        unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
        na.omit()
      #clean stopwords in bigrmas
      bigrams_separated <- tweet_bigrams %>%
        separate(bigram, c("word1", "word2"), sep = " ")
      bigrams_filtered <- bigrams_separated %>%
        filter(!word1 %in% stop_words$word) %>%
        filter(!word2 %in% stop_words$word)
      bigram_counts <- bigrams_filtered %>% 
        count(word1, word2, sort = TRUE)
      #plot bigram network graph with bigrams with minimum frequency > input$freq
      bigram_graph <- bigram_counts %>%
        filter(n > input$freq_ba) %>%
        graph_from_data_frame()
      ggraph(bigram_graph, layout = "fr") +
        geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                       arrow = a, end_cap = circle(.07, 'inches')) +
        geom_node_point(color = "lightblue", size = 5) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1,size=2) +
        theme_void()
    })
    ##Part 6:Topic Modeling
    datasetInput <- reactive({
      if (input$group_topic != "All"){
        sql <- "SELECT text,user_id FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
        query  <- sqlInterpolate(pool, sql, group = input$group_topic,date1 = format(input$date3,format = "%Y-%m-%d"),date2=format(input$date4,format="%Y-%m-%d"))
      }
      else {
        sql <- "SELECT text,user_id FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2;"
        query  <- sqlInterpolate(pool, sql,date1 = format(input$date3,format = "%Y-%m-%d"),date2=format(input$date4,format="%Y-%m-%d"))
      }
      df <- dbGetQuery(pool, query)
      tweet5 <- tbl_df(df)
      text5 <- as.character(tweet5$text)
      text5 <- iconv(text5, 'utf-8', 'ascii', sub='')
      clean_tweet5 = gsub("&amp", "", text5)
      clean_tweet5 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet5)
      clean_tweet5 = gsub("@\\w+", "", clean_tweet5)
      clean_tweet5 = gsub("[[:punct:]]", "", clean_tweet5)
      clean_tweet5 = gsub("[[:digit:]]", "", clean_tweet5)
      clean_tweet5 = gsub("http\\w+", "", clean_tweet5)
      clean_tweet5 = gsub("[ \t]{2,}", "", clean_tweet5)
      clean_tweet5 = gsub("^\\s+|\\s+$", "", clean_tweet5)
      tweet5$text <- clean_tweet5
      tidy_tweet5 <- tweet5 %>%
        unnest_tokens(word, text)
      tidy_tweet5 <- tidy_tweet5 %>%
        anti_join(stop_words)
      tidy_tweet_id <- tidy_tweet5 %>%
        count(user_id, word)
      #create dtm according to user id
      new_dtm <- tidy_tweet_id %>%
        cast_dtm(user_id, word, n)
      rowTotals <- apply(new_dtm , 1, sum) #find the sum of words in each Document
      new_dtm <- new_dtm[rowTotals> 0, ]  #remove all docs without words
      #LDA with Gibbs Sampling
      tweet.lda <- LDA(new_dtm, input$topic, method="Gibbs", control=list(seed = SEED))
      #print the n most important words in the topic
      lda.terms <- as.table(terms(tweet.lda,input$words))
    }
    )
    #print the table
    output$table <- renderDataTable({
      datatable(datasetInput(),colnames = c("Index","Topic","Word"))
    })
    #download the table as csv
    output$downloadData <- downloadHandler(
      filename = function() {paste("Top",input$words,"words-", Sys.Date(), ".csv", sep="")},
      content = function(file) {
        write.csv(datasetInput(), file)
        })
}
```


```{r}
shinyApp(ui = ui, server = server)
```

